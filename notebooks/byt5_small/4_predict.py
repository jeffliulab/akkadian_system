import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
import os
import re
import sys
import math
import evaluate # ä½¿ç”¨ evaluate åº“è°ƒç”¨ sacrebleu

# ================= é…ç½®åŒº =================
# æµ‹è¯•é›†æ–‡ä»¶è·¯å¾„
TEST_FILE = "../../data/deep-past-initiative-machine-translation/test.csv"
# åŸå§‹è®­ç»ƒæ¸…æ´—æ–‡ä»¶ (ç”¨äºè®¡ç®—æœ¬åœ°éªŒè¯åˆ†)
TRAIN_FILE = "clean.csv"

# æ¨¡å‹è·¯å¾„
MODEL_PATH = "./final_akkadian_model"

# è¾“å‡ºæ–‡ä»¶
SUBMISSION_FILE = "submission.csv"

# æ¨ç†å‚æ•°
BATCH_SIZE = 32         # 4090 æ˜¾å­˜å¤§ï¼Œæ¨ç†æ—¶å¯ä»¥å¼€å¤§ä¸€ç‚¹
MAX_INPUT_LENGTH = 1024 
MAX_TARGET_LENGTH = 256
BEAM_SIZE = 4           # Beam Search å®½åº¦

# ================= æ¸…æ´—é€»è¾‘ (SourceNormalizer) =================
class SourceNormalizer:
    """é˜¿å¡å¾·è¯­è¾“å…¥ç«¯æ¸…æ´—å™¨ (å¿…é¡»ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´)"""
    def __init__(self):
        self.sub_map = str.maketrans("â‚€â‚â‚‚â‚ƒâ‚„â‚…â‚†â‚‡â‚ˆâ‚‰", "0123456789")

    def normalize(self, text: str) -> str:
        if not isinstance(text, str): return ""
        text = text.replace('á¸«', 'h').replace('á¸ª', 'H')
        text = text.replace('(', '{').replace(')', '}')
        text = text.replace('[... ...]', '@BIGGAP@').replace('...', '@BIGGAP@').replace('[x]', '@GAP@')
        text = re.sub(r'\{(.*?)\}', r'@DET_\1@', text)
        text = text.translate(self.sub_map)
        text = re.sub(r'[!?:;\[\]\(\)Ë¹Ëº/\\<>\.]', '', text)
        text = text.replace('@BIGGAP@', '<big_gap>').replace('@GAP@', '<gap>')
        text = re.sub(r'@DET_(.*?)@', r'{\1}', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

# ================= æ•°æ®é›†ç±» =================
class InferenceDataset(Dataset):
    def __init__(self, ids, texts, normalizer):
        self.ids = ids
        self.texts = texts
        self.normalizer = normalizer

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, idx):
        raw_text = self.texts[idx]
        id_val = self.ids[idx]
        
        # æ¸…æ´— + æ·»åŠ å‰ç¼€
        clean_text = self.normalizer.normalize(raw_text)
        input_text = f"translate Akkadian to English: {clean_text}"
        
        return {"id": id_val, "input_text": input_text}

# ================= è¯„åˆ†å·¥å…· =================
def calculate_score(predictions, references):
    """
    è®¡ç®—æ¯”èµ›æŒ‡æ ‡: Geometric Mean of BLEU and chrF++
    Score = sqrt(BLEU * chrF++)
    """
    try:
        # åŠ è½½æŒ‡æ ‡
        metric_bleu = evaluate.load("sacrebleu")
        metric_chrf = evaluate.load("chrf")
        
        # 1. è®¡ç®— BLEU
        # sacrebleu æœŸæœ› references æ˜¯ list of list
        # refs_for_bleu = [['ref1'], ['ref2'], ...]
        refs_for_bleu = [[r] for r in references]
        bleu_res = metric_bleu.compute(predictions=predictions, references=refs_for_bleu)
        bleu_score = bleu_res['score']
        
        # 2. è®¡ç®— chrF++
        # sacrebleu çš„ chrf å®ç°ä¸­ï¼Œword_order=2 å³ä¸º chrF++
        chrf_res = metric_chrf.compute(predictions=predictions, references=refs_for_bleu, word_order=2)
        chrf_score = chrf_res['score']
        
        # 3. è®¡ç®—å‡ ä½•å¹³å‡
        # é¿å… 0 åˆ†å¯¼è‡´æ•°å­¦é”™è¯¯
        if bleu_score < 0: bleu_score = 0
        if chrf_score < 0: chrf_score = 0
        
        final_score = math.sqrt(bleu_score * chrf_score)
        
        return {
            "geom_mean": final_score,
            "bleu": bleu_score,
            "chrf++": chrf_score
        }
    except Exception as e:
        print(f"âš ï¸ è¯„åˆ†è®¡ç®—å‡ºé”™: {e}")
        return {"geom_mean": 0.0, "bleu": 0.0, "chrf++": 0.0}

# ================= æ¨ç†æ ¸å¿ƒå‡½æ•° =================
def run_inference(model, tokenizer, dataloader, device):
    results = []
    print(f"ğŸ”¥ å¼€å§‹æ¨ç† (Batch Size: {BATCH_SIZE})...")
    
    for batch in tqdm(dataloader, desc="Generating"):
        ids = batch["id"]
        input_texts = batch["input_text"]
        
        # Tokenize
        inputs = tokenizer(
            input_texts, 
            max_length=MAX_INPUT_LENGTH, 
            truncation=True, 
            padding=True, 
            return_tensors="pt"
        ).to(device)
        
        # Generate
        with torch.no_grad():
            generated_ids = model.generate(
                inputs["input_ids"],
                max_length=MAX_TARGET_LENGTH,
                num_beams=BEAM_SIZE,
                early_stopping=True
            )
        
        # Decode
        decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
        
        # Collect
        for id_val, pred in zip(ids, decoded_preds):
            # [ä¿®æ­£] æå– scalar valueï¼Œä¿®å¤ tensor(0) é—®é¢˜
            clean_id = id_val.item() if isinstance(id_val, torch.Tensor) else id_val
            results.append({"id": clean_id, "translation": pred.strip()})
            
    return results

# ================= ä¸»ç¨‹åº =================
def main():
    print(f"{'='*30} Deep Past æ¨ç†ä¸è¯„åˆ†ç³»ç»Ÿ {'='*30}")

    # 1. èµ„æºæ£€æŸ¥
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹: {MODEL_PATH}")
        return

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ è®¾å¤‡: {device}")
    if "cuda" in str(device):
        print(f"   - æ˜¾å¡: {torch.cuda.get_device_name(0)}")

    # 2. åŠ è½½æ¨¡å‹
    print("ğŸ“¥ åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)
        model.to(device)
        model.eval()
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    normalizer = SourceNormalizer()

    # ================= é˜¶æ®µ 1: ç”Ÿæˆ Submission (Test Set) =================
    if os.path.exists(TEST_FILE):
        print(f"\n{'='*10} é˜¶æ®µ 1: ç”Ÿæˆæ¯”èµ›æäº¤æ–‡ä»¶ (Test Set) {'='*10}")
        df_test = pd.read_csv(TEST_FILE)
        
        # åˆ—åé€‚é…
        text_col = 'transliteration'
        if text_col not in df_test.columns:
            possible = [c for c in df_test.columns if 'text' in c.lower() or 'translit' in c.lower()]
            if possible: text_col = possible[0]
        
        print(f"ğŸ“„ æµ‹è¯•é›†: {len(df_test)} æ¡æ ·æœ¬")
        
        test_ds = InferenceDataset(df_test['id'].tolist(), df_test[text_col].tolist(), normalizer)
        test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)
        
        # æ‰§è¡Œæ¨ç†
        test_results = run_inference(model, tokenizer, test_loader, device)
        
        # ä¿å­˜
        sub_df = pd.DataFrame(test_results)
        sub_df.to_csv(SUBMISSION_FILE, index=False)
        print(f"âœ… Submission å·²ç”Ÿæˆ: {SUBMISSION_FILE}")
        print(sub_df.head(3))
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°æµ‹è¯•é›† {TEST_FILE}ï¼Œè·³è¿‡ç”Ÿæˆæ­¥éª¤ã€‚")

    # ================= é˜¶æ®µ 2: æœ¬åœ°éªŒè¯è¯„åˆ† (Validation Set) =================
    if os.path.exists(TRAIN_FILE):
        print(f"\n{'='*10} é˜¶æ®µ 2: è®¡ç®—æœ¬åœ°éªŒè¯åˆ†æ•° (Local CV) {'='*10}")
        print("â„¹ï¸ è¯´æ˜: ä½¿ç”¨è®­ç»ƒæ—¶åˆ’åˆ†å‡ºçš„éªŒè¯é›†(10%)è¿›è¡Œè¯„ä¼°ï¼Œä½œä¸ºLeaderboardåˆ†æ•°çš„å‚è€ƒã€‚")
        
        # åŠ è½½æ¸…æ´—åçš„è®­ç»ƒæ•°æ®
        df_full = pd.read_csv(TRAIN_FILE)
        
        # å¤ç°è®­ç»ƒæ—¶çš„åˆ‡åˆ† (å¿…é¡»ç”¨ç›¸åŒçš„ seed=42)
        from sklearn.model_selection import train_test_split
        _, df_val = train_test_split(df_full, test_size=0.1, random_state=42)
        
        print(f"ğŸ§ª éªŒè¯é›†: {len(df_val)} æ¡æ ·æœ¬")
        
        # å‡†å¤‡æ•°æ®
        # æå– clean.csv ä¸­çš„åŸå§‹å†…å®¹ï¼ˆéœ€è¦å»æ‰ä¹‹å‰å¯èƒ½åŠ çš„å‰ç¼€ï¼‰
        val_texts = df_val['input_text'].apply(lambda x: x.replace("translate Akkadian to English: ", "")).tolist()
        val_refs = df_val['target_text'].tolist()
        
        # è¿™é‡Œä¸éœ€è¦ idï¼Œç”¨ range ä»£æ›¿
        val_ds = InferenceDataset(range(len(val_texts)), val_texts, normalizer)
        val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)
        
        # æ‰§è¡Œæ¨ç†
        val_preds_raw = run_inference(model, tokenizer, val_loader, device)
        val_preds_text = [item['translation'] for item in val_preds_raw]
        
        # è®¡ç®—åˆ†æ•°
        print("ğŸ§® æ­£åœ¨è®¡ç®—åˆ†æ•° (BLEU + chrF++)...")
        scores = calculate_score(val_preds_text, val_refs)
        
        print(f"\n{'*'*40}")
        print(f"ğŸ† æœ¬åœ°éªŒè¯é›†é¢„ä¼°åˆ†æ•° (Local CV Score)")
        print(f"{'*'*40}")
        print(f"   BLEU Score  : {scores['bleu']:.2f}")
        print(f"   chrF++ Score: {scores['chrf++']:.2f}")
        print(f"   ------------------------------")
        print(f"   Geometric Mean: {scores['geom_mean']:.4f}")
        print(f"{'*'*40}\n")
        
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒæ–‡ä»¶ {TRAIN_FILE}ï¼Œæ— æ³•è®¡ç®—æœ¬åœ°åˆ†æ•°ã€‚")

if __name__ == "__main__":
    main()