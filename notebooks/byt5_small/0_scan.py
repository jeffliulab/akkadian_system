import pandas as pd
import os
import re
import glob

# ================= é…ç½®åŒº =================
# è‡ªåŠ¨å®šä½åˆ° deep-past æ•°æ®é›†ç›®å½•
CURRENT_DIR = os.getcwd()
DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(CURRENT_DIR)), "data", "deep-past-initiative-machine-translation")

# ================= æ ¸å¿ƒåˆ†æå¼•æ“ =================

def analyze_text_fingerprint(series, col_name):
    """
    æ·±åº¦æ–‡æœ¬æ³•åŒ»åˆ†æï¼šåˆ†æä¸€åˆ—æ–‡æœ¬çš„ç¬¦å·ç‰¹å¾
    """
    # å–å‰ 2000 è¡Œéç©ºæ•°æ®ä½œä¸ºæ ·æœ¬ï¼Œå…¼é¡¾é€Ÿåº¦ä¸ä»£è¡¨æ€§
    valid_data = series.dropna().astype(str)
    if valid_data.empty:
        return "    - [ç©ºåˆ—]"
    
    sample_text = " ".join(valid_data.head(2000))
    
    # 1. æ‹¬å·é£æ ¼ (å…³é”®ï¼šå†³å®šæ˜¯å¦éœ€è¦åš () -> {} æ˜ å°„)
    round_brackets = len(re.findall(r'\(.*?\)', sample_text))
    curly_brackets = len(re.findall(r'\{.*?\}', sample_text))
    
    # 2. å˜éŸ³ç¬¦å· (å…³é”®ï¼šå†³å®šæ˜¯å¦éœ€è¦ä¿æŠ¤ Å¡, á¹£, á¸«)
    h_chars = len(re.findall(r'[á¸«á¸ª]', sample_text))
    s_chars = len(re.findall(r'[Å¡Å ]', sample_text))
    
    # 3. ä¸‹æ ‡ (å…³é”®ï¼šå†³å®šæ˜¯å¦éœ€è¦ Unicode å½’ä¸€åŒ–)
    uni_subs = len(re.findall(r'[â‚€-â‚‰]', sample_text))
    
    # 4. ç¼ºæŸæ ‡è®°
    gaps = sample_text.count('[x]') + sample_text.count('...')
    
    report = f"    [ğŸ”¬ åˆ—åˆ†æ: {col_name}]\n"
    report += f"      - ç¡®å®šå€¼é£æ ¼ : åœ†æ‹¬å·={round_brackets} vs èŠ±æ‹¬å·={curly_brackets}"
    
    if round_brackets > 0 and curly_brackets == 0:
        report += " âš ï¸ (éœ€å¯ç”¨ V11 è½¬æ¢)"
    elif curly_brackets > 0:
        report += " âœ… (æ ‡å‡†æ ¼å¼)"
        
    report += f"\n      - æ–‡æ˜æŒ‡çº¹   : á¸«/á¸ª={h_chars}, Å¡/Å ={s_chars}, ä¸‹æ ‡={uni_subs}"
    report += f"\n      - ç¼ºæŸæ ‡è®°   : {gaps} å¤„"
    
    return report

def scan_file(filepath):
    filename = os.path.basename(filepath)
    print(f"\n>>> æ­£åœ¨æ‰«ææ–‡ä»¶: {filename}")
    
    try:
        # å¼ºåˆ¶ UTF-8 è¯»å–
        df = pd.read_csv(filepath, encoding='utf-8')
        print(f"    - å½¢çŠ¶: {df.shape}")
        print(f"    - åˆ—å: {df.columns.tolist()}")
        
        # æ™ºèƒ½æ¢æµ‹ï¼šå¯»æ‰¾åŒ…å« 'translit', 'transla', 'text', 'spelling' çš„åˆ—è¿›è¡Œæ·±åº¦åˆ†æ
        target_cols = [c for c in df.columns if any(x in c.lower() for x in ['translit', 'transla', 'text', 'spelling', 'form'])]
        
        if target_cols:
            print(f"    - å‘½ä¸­æ ¸å¿ƒæ–‡æœ¬åˆ—: {target_cols}")
            for col in target_cols:
                print(analyze_text_fingerprint(df[col], col))
        else:
            print("    - (æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„é˜¿å¡å¾·è¯­/è‹±è¯­æ–‡æœ¬åˆ—ï¼Œè·³è¿‡æ·±åº¦åˆ†æ)")
            
        # ä»·å€¼è¯„ä¼°
        cols_lower = [c.lower() for c in df.columns]
        has_source = any('translit' in c for c in cols_lower)
        has_target = any('transla' in c or 'eng' in c for c in cols_lower)
        
        if has_source and has_target:
            print(f"    ğŸŒŸ [é«˜ä»·å€¼] å‘ç°æ½œåœ¨çš„å¹³è¡Œè¯­æ–™ (Source + Target)!")
        elif has_source:
            print(f"    ğŸ”¶ [ä¸­ä»·å€¼] ä»…å‘ç°è½¬å†™æ–‡æœ¬ (å¯ç”¨ä½œé¢„è®­ç»ƒ/å•è¯­æ•°æ®)")
            
        return df  # è¿”å› DataFrame ç”¨äºåç»­å…³è”åˆ†æ

    except Exception as e:
        print(f"    âŒ è¯»å–å¤±è´¥: {e}")
        return None

def check_relationships(data_map):
    print(f"\n{'='*20} ğŸ”— æ–‡ä»¶å…³è”æ€§å›¾è°±åˆ†æ {'='*20}")
    
    # 1. æ ¸å¿ƒå…³è”: Train <-> Sentences
    if 'train.csv' in data_map and 'Sentences_Oare_FirstWord_LinNum.csv' in data_map:
        train = data_map['train.csv']
        sent = data_map['Sentences_Oare_FirstWord_LinNum.csv']
        
        # æ£€æŸ¥ train.oare_id å’Œ sent.text_uuid
        common = set(train['oare_id']).intersection(set(sent['text_uuid']))
        coverage = len(common) / len(train) * 100
        print(f"  [Train <-> Sentences]")
        print(f"    - å…³è”é”®: train['oare_id'] == sent['text_uuid']")
        print(f"    - åŒ¹é… ID æ•°: {len(common)} (è¦†ç›–ç‡: {coverage:.2f}%)")
        if coverage < 10:
            print("    âš ï¸ è­¦å‘Š: è¦†ç›–ç‡æä½ï¼Œè¯´æ˜å¤§éƒ¨åˆ†è®­ç»ƒé›†æ–‡æ¡£æ²¡æœ‰å¯¹åº”çš„å¥å­åˆ‡åˆ†æ•°æ®ï¼")
    
    # 2. æ½œåœ¨å…³è”: Published Texts (å¦‚æœæœ‰)
    if 'published_texts.csv' in data_map:
        pub = data_map['published_texts.csv']
        print(f"  [Published Texts æ¦‚å†µ]")
        print(f"    - æ€»è¡Œæ•°: {len(pub)}")
        # çœ‹çœ‹æœ‰æ²¡æœ‰ ID å¯ä»¥è·Ÿå…¶ä»–è¡¨è¿
        if 'id' in pub.columns or 'uuid' in pub.columns:
            print(f"    - å¯èƒ½çš„ä¸»é”®: {[c for c in pub.columns if 'id' in c.lower()]}")

# ================= ä¸»ç¨‹åº =================

def main():
    print(f"{'='*30} å…¨åŸŸæ•°æ®èµ„äº§æ³•åŒ»çº§æ‰«æ {'='*30}")
    print(f"ç›®æ ‡ç›®å½•: {DATA_DIR}")
    
    if not os.path.exists(DATA_DIR):
        print(f"âŒ è‡´å‘½é”™è¯¯: ç›®å½•ä¸å­˜åœ¨ï¼")
        return

    # è·å–æ‰€æœ‰ csv æ–‡ä»¶
    csv_files = glob.glob(os.path.join(DATA_DIR, "*.csv"))
    print(f"å‘ç° {len(csv_files)} ä¸ª CSV æ–‡ä»¶å¾…å®¡è®¡ã€‚\n")
    
    data_map = {}
    
    # é€ä¸ªæ‰«æ
    for fpath in csv_files:
        filename = os.path.basename(fpath)
        df = scan_file(fpath)
        if df is not None:
            data_map[filename] = df
            
    # å…³è”åˆ†æ
    check_relationships(data_map)
    
    print(f"\n{'='*30} æ‰«æç»“æŸ {'='*30}")
    print(">>> æ ¸å¿ƒè¡ŒåŠ¨æŒ‡å—:")
    print("1. å¦‚æœ Train/Test çš„æ‹¬å·é£æ ¼æ˜¯ 'åœ†æ‹¬å·'ï¼Œé¢„å¤„ç†å¿…é¡»åŒ…å« replace('(', '{')ã€‚")
    print("2. å¦‚æœ Sentences çš„è¦†ç›–ç‡ä½ï¼Œéœ€è¦æ£€æŸ¥æ˜¯å¦å¯ä»¥é€šè¿‡ published_texts.csv è¡¥å……æ•°æ®ã€‚")
    print("3. å¦‚æœå‘ç° 'ä¸­ä»·å€¼' æ–‡ä»¶ï¼Œè€ƒè™‘å°†å…¶åŠ å…¥é¢„è®­ç»ƒè¯­æ–™åº“ã€‚")

if __name__ == "__main__":
    main()