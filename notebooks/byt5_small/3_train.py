import sys
import os
import math

# --- 1. ç¯å¢ƒè‡ªæ£€ ---
try:
    import torch
except ImportError:
    print("âŒ é”™è¯¯: æœªå®‰è£… PyTorchã€‚")
    sys.exit(1)

def check_environment():
    print(f"\n{'='*30} ç¯å¢ƒç¡¬ä»¶è‡ªæ£€ (V10 Ultimate) {'='*30}")
    
    if not torch.cuda.is_available():
        print("âŒ è‡´å‘½é”™è¯¯: æœªæ£€æµ‹åˆ° GPUï¼")
        sys.exit(1)
    else:
        try:
            device_name = torch.cuda.get_device_name(0)
            total_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"âœ… GPU çŠ¶æ€: æ­£å¸¸ (CUDA Active)")
            print(f"   - æ˜¾å¡å‹å·: {device_name}")
            print(f"   - æ˜¾å­˜å¤§å°: {total_mem:.2f} GB")
            
            if "4090" in device_name:
                print("ğŸš€ æ£€æµ‹åˆ° RTX 4090ï¼BF16 ç¨³å¥æ¨¡å¼å·²æ¿€æ´»ã€‚")
                
        except Exception as e:
            print(f"âš ï¸ GPU ä¿¡æ¯è·å–å¤±è´¥: {e}")
            
    print(f"{'='*80}\n")

check_environment()

import pandas as pd
import numpy as np
from datasets import Dataset
import evaluate
from transformers import (
    AutoTokenizer, 
    AutoModelForSeq2SeqLM, 
    DataCollatorForSeq2Seq, 
    Seq2SeqTrainingArguments, 
    Seq2SeqTrainer,
    TrainerCallback
)

# ================= é…ç½®åŒº =================
INPUT_FILE = "clean.csv"
MODEL_CHECKPOINT = "google/byt5-small"
OUTPUT_DIR = "./byt5_akkadian_finetuned"
FINAL_MODEL_DIR = "./final_akkadian_model"

MAX_INPUT_LENGTH = 1024
MAX_TARGET_LENGTH = 256
BATCH_SIZE = 8          
LEARNING_RATE = 1e-4    
NUM_EPOCHS = 50         
LOGGING_STEPS = 10      

# ================= ğŸ›¡ï¸ å®‰å…¨ç†”æ–­å™¨ (ä¿ç•™) =================
class SafetyCallback(TrainerCallback):
    """
    å®æ—¶ç›‘æ§è®­ç»ƒçŠ¶æ€ï¼Œä¸€æ—¦å‘ç° Loss å¼‚å¸¸ (NaN/Inf/0.0)ï¼Œç«‹å³ç»ˆæ­¢è®­ç»ƒã€‚
    """
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is None: return
        
        if "loss" in logs:
            loss_val = logs["loss"]
            
            # 1. æ£€æŸ¥ NaN
            if math.isnan(loss_val):
                print(f"\n\nâŒ [ç†”æ–­è§¦å‘] æ£€æµ‹åˆ° Loss = NaN (æ¢¯åº¦çˆ†ç‚¸)ï¼")
                sys.exit(1)
            
            # 2. æ£€æŸ¥ Inf
            if math.isinf(loss_val):
                print(f"\n\nâŒ [ç†”æ–­è§¦å‘] æ£€æµ‹åˆ° Loss = Inf (æ•°å€¼æº¢å‡º)ï¼")
                sys.exit(1)
                
            # 3. æ£€æŸ¥ 0.0
            if loss_val == 0.0 and state.global_step < 100:
                print(f"\n\nâŒ [ç†”æ–­è§¦å‘] æ£€æµ‹åˆ° Loss = 0.0 (å¼‚å¸¸å½’é›¶)ï¼")
                sys.exit(1)

def train():
    print(f"{'='*30} å¯åŠ¨è®­ç»ƒå¼•æ“ (V10) {'='*30}")
    
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° {INPUT_FILE}")
        return

    df = pd.read_csv(INPUT_FILE)
    raw_dataset = Dataset.from_pandas(df)
    
    # 9:1 åˆ’åˆ†
    split_dataset = raw_dataset.train_test_split(test_size=0.1, seed=42)
    print(f"ğŸ“Š æ•°æ®æ¦‚å†µ:\n  - è®­ç»ƒé›†: {len(split_dataset['train'])} æ¡\n  - éªŒè¯é›†: {len(split_dataset['test'])} æ¡")

    print(f"ğŸš€ åŠ è½½åŸºåº§æ¨¡å‹: {MODEL_CHECKPOINT}...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)
    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)

    def preprocess_function(examples):
        inputs = examples["input_text"]
        targets = examples["target_text"]
        model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)
        labels = tokenizer(text_target=targets, max_length=MAX_TARGET_LENGTH, truncation=True)
        model_inputs["labels"] = labels["input_ids"]
        return model_inputs

    print("âš™ï¸ æ­£åœ¨å¯¹é½ä¸ç¼–ç æ•°æ®...")
    tokenized_datasets = split_dataset.map(preprocess_function, batched=True)

    metric = evaluate.load("sacrebleu")

    # [åŠŸèƒ½å‡çº§] ä¿®å¤ chr() æŠ¥é”™çš„ compute_metrics
    def compute_metrics(eval_preds):
        preds, labels = eval_preds
        if isinstance(preds, tuple):
            preds = preds[0]
            
        try:
            # å°è¯•æ­£å¸¸è§£ç 
            decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
            
            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

            decoded_preds = [pred.strip() for pred in decoded_preds]
            decoded_labels = [[label.strip()] for label in decoded_labels]

            result = metric.compute(predictions=decoded_preds, references=decoded_labels)
            return {"bleu": result["score"]}
        
        except ValueError as e:
            # æ•è· chr() not in range ç­‰è§£ç é”™è¯¯
            if "chr()" in str(e) or "range" in str(e):
                print(f"\nâš ï¸ [Warning] è¯„ä¼°è·³è¿‡: æ£€æµ‹åˆ°éæ³•å­—ç¬¦ç”Ÿæˆ (ByT5 æ—©æœŸå¸¸è§éœ‡è¡)ï¼Œä¸å½±å“è®­ç»ƒã€‚")
                return {"bleu": 0.0}
            else:
                # å…¶ä»–é”™è¯¯åˆ™æ‰“å°è¯¦æƒ…
                print(f"\nâš ï¸ [Warning] è¯„ä¼°æœªçŸ¥é”™è¯¯: {e}")
                return {"bleu": 0.0}

    print("ğŸ“ˆ é…ç½®è®­ç»ƒå‚æ•° (BF16 + Auto-Kill enabled)...")
    args = Seq2SeqTrainingArguments(
        output_dir=OUTPUT_DIR,
        eval_strategy="epoch",            
        save_strategy="epoch",            
        learning_rate=LEARNING_RATE,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        weight_decay=0.01,
        save_total_limit=2,
        num_train_epochs=NUM_EPOCHS,
        predict_with_generate=True,       
        
        # 4090 æ ¸å¿ƒé…ç½®: BF16
        bf16=True,                        
        fp16=False,                       
        
        generation_max_length=256,        
        logging_steps=LOGGING_STEPS,      
        load_best_model_at_end=True,      
        metric_for_best_model="bleu",     
        greater_is_better=True,
        report_to="none"
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["test"],
        data_collator=DataCollatorForSeq2Seq(tokenizer, model=model, padding=True),
        processing_class=tokenizer,
        compute_metrics=compute_metrics,
        callbacks=[SafetyCallback]  # <--- ä¿ç•™å®‰å…¨ç†”æ–­å™¨
    )

    # [æ–°å¢åŠŸèƒ½] è‡ªåŠ¨æ£€æµ‹æ–­ç‚¹ï¼Œæ”¯æŒ Resume
    last_checkpoint = None
    if os.path.isdir(OUTPUT_DIR):
        # å¯»æ‰¾ checkpoint-XXX æ–‡ä»¶å¤¹
        checkpoints = [os.path.join(OUTPUT_DIR, d) for d in os.listdir(OUTPUT_DIR) if d.startswith("checkpoint")]
        if checkpoints:
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæ‰¾æœ€æ–°çš„
            checkpoints.sort(key=os.path.getmtime)
            last_checkpoint = checkpoints[-1]
            print(f"â™»ï¸ æ£€æµ‹åˆ°è®­ç»ƒå­˜æ¡£ï¼Œå°†ä»æ–­ç‚¹æ¢å¤: {last_checkpoint}")

    print("ğŸ”¥ ç‚¹ç«èµ·é£ï¼å¼€å§‹è®­ç»ƒ...")
    try:
        # å¦‚æœæœ‰ checkpoint å°±ç»­è®­ï¼Œæ²¡æœ‰å°±é‡å¤´å¼€å§‹
        trainer.train(resume_from_checkpoint=last_checkpoint)
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return

    print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹è‡³: {FINAL_MODEL_DIR}")
    model.save_pretrained(FINAL_MODEL_DIR)
    tokenizer.save_pretrained(FINAL_MODEL_DIR)
    print("âœ… è®­ç»ƒå…¨æµç¨‹ç»“æŸï¼")

if __name__ == "__main__":
    train()